https://www.bilibili.com/video/BV164411b7dx?p=2&spm_id_from=pageDriver

# 吴恩达机器学习课程笔记

## 什么是机器学习

Tom Mitchell在1997年提出的“ETP定义”：A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.

E：Experience，经验

T：Task，任务

P：Performance measure，性能测度

从E来提高做T的P

**重点：学习怎么使用正确机器学习算法 来完成任务**

学习算法中主要的两类：

+ supevised learning  监督学习 （教计算机学习）
+ unsupervised learning           （让计算机自己学习）

其他类型：

+ 
  reinforcement learmning  强化学习
+ recommender systems  推荐系统

## 监督学习

每个例子都有正确的答案

+ 分类问题

  告诉计算机什么是0 和 1  然后给一个数据  告诉你这个是是0还是1

+ 回归问题

  eg：分析不同地区 卖房 可以卖到多少钱

## 无监督学习

给一个数据集  它没有标签或者标签相同 其中之一 聚类算法  计算器自动分类 

eg：谷歌新闻页面 组合新闻链接 

eg：从录音中分离 不同的人声 或者分离伴奏和人声

octave—科学计算软件

## 模型描述

+ 一个训练集
+ 训练数
+ 特征 给的值
+ 目标值

预测房价的例子  ：

![image-20211108210406083](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211108210406083.png)

预测房价的例子  ：监督学习—线性回归

一行就是一个训练样本

x^1 y^1   这里1 指的是索引

得出的结果 就是叫假设函数

## 代价函数

![20211108214222](https://gitee.com/dong2645981073/picture-summary/raw/master//image/20211108214222.png)



假设函数，参数，代价函数，优化目标：

![image-20211108222305107](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211108222305107.png)

1. 简化了房价的例子的参数 只有一个斜率参数θ1

![image-20211108221850671](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211108221850671.png)

2. 两个参数 都有：

![image-20211108223437244](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211108223437244.png)



![image-20211108223446980](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211108223446980.png)

（图A）一个参数 和两个参数的 代价函数

![image-20211108223454288](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211108223454288.png)

代价函数画成  等高线图后的样子  一个点 代表一个h（x） 假设函数

椭圆的“同心点” 拟合程度较高

我们需要软件来帮助我们找到最低点和对应的θ1和θ0

## 梯度下降法

1. 梯度下降法来最小化任意函数：

![image-20211108225451754](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211108225451754.png)

不同的起始点 找到的最低点可能不同（一般不是这样 应该是一个**凸**函数  像上面图一）

![image-20211109172655125](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211109172655125.png)

2. 梯度下降算法

![image-20211109172327978](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211109172327978.png)



“正确的梯度下降要同步更新 右侧是不正确的"

![image-20211109175015782](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211109175015782.png)

![image-20211109175310119](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211109175310119.png)



3. α是控制下降速率  

+ α小的话 下降比较

+ α大的话  可能导致找不到最小点或者发散

算法中的导数项 通俗地说是开控制下降的方向

导数项 是慢慢减小的 最后是0  所以梯度下降将自动采取较小的幅度



整合模型和算法下·

4. ![image-20211109183608749](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211109183608749.png)

   右的线性回归模型 带入梯度算法中  导数项和代价函数如下图



![image-20211109183657678](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211109183657678.png)



不断重复下面

![image-20211109185810332](https://gitee.com/dong2645981073/picture-summary/raw/master//image/image-20211109185810332.png)

*上面的方法是全览了整个训练集 的梯度下降 所以它又叫”batch“算法*

*(还有没有全览了整个训练集的算法）*

